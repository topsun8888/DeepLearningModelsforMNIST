{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20e5d854f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[5].reshape([28, 28]))\n",
    "print(mnist.train.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tr = mnist.train.images.shape[0]# number of training samples\n",
    "n_ts = mnist.test.images.shape[0]#number of testing samples\n",
    "n_pixel = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Nomalization For CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decay = 0.9\n",
    "epsilon = 0.001\n",
    "def conv_relu_pool(x, kernel_shape, bias_shape, is_training):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = weight_variable(kernel_shape)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = bias_variable(bias_shape)\n",
    "    \n",
    "    # Create two new parameters, gamma and beta (shift)\n",
    "    #The bias for batch normalization\n",
    "    gamma = tf.Variable(tf.ones(bias_shape))\n",
    "    #\n",
    "    beta = tf.Variable(tf.zeros(bias_shape))\n",
    "    \n",
    "    conv = conv2d(x, weights)    \n",
    "    z = conv + biases\n",
    "    \n",
    "    moving_mean = tf.Variable(tf.zeros([z.get_shape()[-1]]), trainable=False)\n",
    "    moving_var = tf.Variable(tf.ones([z.get_shape()[-1]]), trainable=False)\n",
    "    axis = list(range(len(z.get_shape()) - 1))\n",
    "    #If training\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(z, axis)\n",
    "        train_mean = tf.assign(moving_mean,\n",
    "                               moving_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(moving_var,\n",
    "                              moving_var * decay + batch_var * (1 - decay))\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            bn = tf.nn.batch_normalization(z,\n",
    "                                           batch_mean, batch_var, beta, gamma, epsilon)    \n",
    "    else:\n",
    "        bn = tf.nn.batch_normalization(z, moving_mean, moving_var, beta, gamma, epsilon)\n",
    "    #Activation    \n",
    "    relu = tf.nn.relu(bn)\n",
    "    pool = max_pool_2x2(relu)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer(x, keep_prob, is_training):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape, is_training)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape, is_training)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.variable_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob, True)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Adam Optimizer\n",
    "    train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 124.399\n",
      "Loss: 0.370755\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "     \n",
    "    saved_model = saver.save(sess, 'temp/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph_cnn.as_default() as g:\n",
    "    y = cnnLayer(x, keep_prob, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from temp\\model.ckpt\n",
      "Testing Accuracy： 0.981\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state('temp')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Below code was referred to http://danijar.com/structuring-your-tensorflow-models/\n",
    "import functools\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__#Get function name\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)#Keep the original function\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):#If the attribute not exist\n",
    "            with tf.variable_scope(name, *args, **kwargs):#Add scope name\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)#otherwise return the attribute\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一个MNIST分类的基类\n",
    "class MnistModel:\n",
    "    '''Define a basic model for MNIST image classification, the model\n",
    "    Provides graph structure of tensorflow'''\n",
    "    \n",
    "    def __init__(self, input_holder, target_holder, is_training, keep_prob):\n",
    "        self.input_holder = input_holder\n",
    "        self.target_holder = target_holder\n",
    "        self.is_training = is_training\n",
    "        self.num_pixel = 784\n",
    "        self.num_class = 10\n",
    "        self.keep_prob = keep_prob\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.accuracy\n",
    "        print('Model Initialized!') \n",
    "    \n",
    "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
    "    def prediction(self):\n",
    "        return None\n",
    "    \n",
    "    @define_scope\n",
    "    def loss(self):\n",
    "        return None\n",
    "    \n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        return None\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:https://github.com/ry/tensorflow-resnet/blob/master/resnet.py\n",
    "https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "https://r2rt.com/implementing-batch-normalization-in-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.training import moving_averages\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "class CnnMnistModel(MnistModel):\n",
    "    #重构某些函数\n",
    "    \n",
    "    @define_scope(initializer=tf.contrib.slim.xavier_initializer())\n",
    "    def prediction(self):\n",
    "        #定义权重和偏置参数变量\n",
    "        logits = self.cnnLayer\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        cross_entropy = self.loss\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        return optimizer.minimize(cross_entropy)\n",
    "    \n",
    "    @define_scope\n",
    "    def loss(self):\n",
    "        cross_entropy = -tf.reduce_mean(self.target_holder*\n",
    "                                       tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        correct_prediction = tf.equal(tf.argmax(self.target_holder,1), \n",
    "                                      tf.argmax(self.prediction,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        return accuracy\n",
    "    \n",
    "    @define_scope\n",
    "    def correct_num(self):\n",
    "        '''Count correct predictions for testing part'''\n",
    "        #labels = tf.one_hot(self.label_holder, self.num_class, 1, 0)\n",
    "        #correct_prediction = tf.equal(tf.argmax(labels,1), \n",
    "                                      #tf.argmax(self.prediction,1))\n",
    "        #correct_ones = tf.reduce_sum(tf.cast(correct_prediction, \"float\"))\n",
    "        logits = self.cnnLayer\n",
    "        labels = tf.argmax(self.target_holder, 1)\n",
    "        eval_correct=tf.nn.in_top_k(logits,labels,1)\n",
    "        return eval_correct\n",
    "    \n",
    "    @define_scope\n",
    "    def cnnLayer(self):\n",
    "        #Note, we need to share weights here, so variable_scope \n",
    "        #should be specified\n",
    "        x_image = tf.reshape(self.input_holder, [-1,28,28,1])\n",
    "        #First Conv\n",
    "        with tf.variable_scope('hidden1'):\n",
    "            kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "            h_pool1 = self.conv_relu_bn_pool(x_image, kernel_shape, bias_shape)\n",
    "            #Second variable_scope\n",
    "        with tf.variable_scope('hidden2'):\n",
    "            kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "            h_pool2 = self.conv_relu_bn_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "        #Fully Connected Layer\n",
    "        with tf.variable_scope('fully_conn'):\n",
    "            W_fc1 = self._get_variable(name='weights', shape=[7 * 7 * 64, 1024])\n",
    "            b_fc1 = self._get_variable(name='bias', shape=[1024])\n",
    "            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
    "        #Softmax Layer\n",
    "        with tf.variable_scope('softmax_layers'):\n",
    "            W_fc2 = self._get_variable(name='weights', shape=[1024, 10])\n",
    "            b_fc2 = self._get_variable(name='bias', shape=[10])\n",
    "            logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    #卷积函数\n",
    "    def conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    #池化函数\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_variable(self, name,\n",
    "                  shape,\n",
    "                  initializer=None,\n",
    "                  weight_decay=0.0,\n",
    "                  trainable=True):\n",
    "        \"A little wrapper around tf.get_variable to do weight decay and add to\"\n",
    "        \"resnet collection\"   \n",
    "        return tf.get_variable(name,\n",
    "                           shape=shape,\n",
    "                           initializer=initializer,\n",
    "                           dtype=tf.float32,\n",
    "                           trainable=trainable)\n",
    "\n",
    "    def conv_relu_bn_pool(self, x, kernel_shape, bias_shape):\n",
    "        # Create variable named \"weights\".\n",
    "        initializer = tf.random_uniform_initializer(-0.01, 0.01)\n",
    "        weights = self._get_variable('conv_weights', kernel_shape, \n",
    "                                     initializer=initializer)\n",
    "        # Create variable named \"biases\".\n",
    "        biases = self._get_variable('conv_bias', bias_shape, \n",
    "                                     initializer=initializer)\n",
    "        conv = self.conv2d(x, weights)\n",
    "        bn = self.batch_normalization(conv+ biases)\n",
    "        relu = tf.nn.relu(bn)\n",
    "        pool = self.max_pool_2x2(relu)\n",
    "        return pool\n",
    "    \n",
    "\n",
    "\n",
    "    def batch_normalization(self, x):\n",
    "        x_shape = x.get_shape()\n",
    "        params_shape = x_shape[-1:]\n",
    "        axis = list(range(len(x_shape) - 1))\n",
    "        #The bias for batch normalization\n",
    "        beta = self._get_variable('beta',\n",
    "                             params_shape,\n",
    "                             initializer=tf.zeros_initializer)\n",
    "        #The scale of batch normalization\n",
    "        gamma = self._get_variable('gamma',\n",
    "                              params_shape,\n",
    "                              initializer=tf.ones_initializer)\n",
    "        #Record moving average for testing\n",
    "        moving_mean = self._get_variable('moving_mean',\n",
    "                                    params_shape,\n",
    "                                    initializer=tf.zeros_initializer,\n",
    "                                    trainable=False)\n",
    "        #Record moving average for testing\n",
    "        moving_variance = self._get_variable('moving_variance',\n",
    "                                    params_shape,\n",
    "                                    initializer=tf.ones_initializer,\n",
    "                                    trainable=False)  \n",
    "        # These ops will only be preformed when training.\n",
    "        mean, variance = tf.nn.moments(x, axis)\n",
    "        #Update moving averages\n",
    "        update_moving_mean = moving_averages.assign_moving_average(moving_mean,\n",
    "                                                               mean, 0.9997)\n",
    "        update_moving_variance = moving_averages.assign_moving_average(moving_variance, \n",
    "                                                                       variance, 0.9997)\n",
    "        tf.add_to_collection(\"UPDATE_OPS_COLLECTION\", update_moving_mean)\n",
    "        tf.add_to_collection(\"UPDATE_OPS_COLLECTION\", update_moving_variance)        \n",
    "        if self.is_training:\n",
    "            mean, variance = mean, variance\n",
    "        else:\n",
    "            mean, variance = moving_mean, moving_variance \n",
    "        x = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.001)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!\n",
      "Model Initialized!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "batch_size = 64\n",
    "num_pixel = 784\n",
    "num_class = 10\n",
    "tf.reset_default_graph() \n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    #定义feed数据\n",
    "    #定义输入数据\n",
    "    input_holder = tf.placeholder(tf.float32, [None, num_pixel])\n",
    "    #输入数据对应标签\n",
    "    target_holder = tf.placeholder(tf.float32, [None,num_class])\n",
    "    initializer = tf.random_uniform_initializer(-0.01, 0.01)\n",
    "    with tf.variable_scope('CNN', initializer=initializer):\n",
    "        train_model = CnnMnistModel(input_holder, target_holder, True, 0.5)\n",
    "    with tf.variable_scope('CNN', reuse=True, initializer=initializer):\n",
    "        test_model = CnnMnistModel(input_holder, target_holder, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.33037 Accuracy:0.06250\n",
      "Loss:0.00711 Accuracy:0.98438\n",
      "Testing Accuracy： 0.1063\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #Train the data\n",
    "    for _ in range(epochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {input_holder : batch_data, target_holder: batch_labels}\n",
    "            #Train\n",
    "            _, l, a = sess.run([train_model.optimize, train_model.loss, train_model.accuracy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                print('Loss:{:.5f}'.format(l), 'Accuracy:{:.5f}'.format(a))\n",
    "    \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {input_holder : batch_data, target_holder: batch_labels}\n",
    "        cp = sess.run(test_model.correct_num, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
