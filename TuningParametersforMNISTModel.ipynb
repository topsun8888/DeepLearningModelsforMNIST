{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to leverage toy examples to illustrate how to build linear model, deep neural networks, convolutional nueral networds as well as recurrent neural networks by [Tensorflow](www.tensorflow.org). The dataset is famous handwritten digits dataset, namely [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 55000 items of training data, and 10000 items of testing data. Let's take a look at the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6abb8f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[5].reshape([28, 28]))\n",
    "print(mnist.train.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX4gKMIPBPX/EhqoeSQulYEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a6abc1dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape([28, 28]))\n",
    "print(mnist.train.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tr = mnist.train.images.shape[0]# number of training samples\n",
    "n_ts = mnist.test.images.shape[0]#number of testing samples\n",
    "n_pixel = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、Choose Right Optimizer \n",
    "\n",
    "Apart from batch gradient descending method, which tends to be slow an memory-consuming, there are several other gradient descending methods which can make train process fast. For example, SGD optimizer feed the model a small number of samples each time during the iteration, and Adam optimizer can adjust itself to accelerate learning. For more information, we can join Andrew Ng.'s deep learning course: https://www.coursera.org/learn/deep-neural-network . \n",
    "\n",
    "### 1. 1 Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create weights\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "  return tf.Variable(initial, name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial, name='biases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu_pool(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = weight_variable(kernel_shape)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = bias_variable(bias_shape)\n",
    "    conv = conv2d(input, weights)\n",
    "    relu = tf.nn.relu(conv + biases)\n",
    "    pool = max_pool_2x2(relu)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer(x, keep_prob):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [5, 5, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [5, 5, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.name_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_method(method, cross_entropy):\n",
    "    if method == 'adam':\n",
    "        train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)\n",
    "    elif method == 'sgd':\n",
    "        #Gradient Descending Optimizer\n",
    "        global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "        starter_learning_rate = 0.0005\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)\n",
    "        #train_step = tf.train.GradientDescentOptimizer(0.0005).minimize(cross_entropy)\n",
    "    elif method == 'rmsprop':\n",
    "        train_step = tf.train.RMSPropOptimizer(learning_rate=0.0005, decay=0.95).minimize(cross_entropy)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph_cnn.as_default() as g:\n",
    "    #Adam Optimizer\n",
    "    train_step = optimize_method('adam', cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "                \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.test.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_cnn2 = tf.Graph()\n",
    "with graph_cnn2.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph_cnn2.as_default() as g:\n",
    "    #Sgd Optimizer\n",
    "    train_step = optimize_method('sgd', cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 147.356\n",
      "Loss: 146.352\n",
      "Loss: 20.7495\n",
      "Loss: 6.21662\n",
      "Loss: 3.75816\n",
      "Loss: 4.20633\n",
      "Loss: 3.72263\n",
      "Loss: 2.21412\n",
      "Loss: 1.09152\n",
      "Loss: 4.7276\n",
      "Loss: 4.4408\n",
      "Loss: 0.959433\n",
      "Loss: 0.727336\n",
      "Loss: 0.384447\n",
      "Loss: 0.942069\n",
      "Loss: 0.709076\n",
      "Loss: 0.394239\n",
      "Loss: 0.118397\n",
      "Loss: 0.620972\n",
      "Loss: 0.394562\n",
      "Loss: 1.19378\n",
      "Loss: 1.96052\n",
      "Loss: 0.930474\n",
      "Loss: 0.0656988\n",
      "Loss: 2.3838\n",
      "Loss: 0.616717\n",
      "Loss: 0.360548\n",
      "Loss: 0.121471\n",
      "Loss: 0.131276\n",
      "Loss: 0.18342\n",
      "Loss: 0.114645\n",
      "Loss: 0.351043\n",
      "Loss: 0.678493\n",
      "Loss: 0.0684983\n",
      "Loss: 0.262424\n",
      "Loss: 0.254962\n",
      "Loss: 0.334311\n",
      "Loss: 0.514812\n",
      "Loss: 0.227101\n",
      "Loss: 0.33369\n",
      "Loss: 0.13883\n",
      "Loss: 0.360746\n",
      "Loss: 0.547493\n",
      "Loss: 0.328938\n",
      "Loss: 0.285729\n",
      "Loss: 0.610307\n",
      "Loss: 0.0187569\n",
      "Loss: 0.0537181\n",
      "Loss: 0.310639\n",
      "Loss: 0.206579\n",
      "Loss: 0.140233\n",
      "Loss: 0.24201\n",
      "Loss: 2.10571\n",
      "Loss: 0.41685\n",
      "Loss: 0.150397\n",
      "Loss: 0.0848864\n",
      "Loss: 0.163611\n",
      "Loss: 0.163637\n",
      "Loss: 0.0790276\n",
      "Loss: 0.393038\n",
      "Testing Accuracy： 0.9984\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 30\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn2) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "                \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RMSProp Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_cnn3 = tf.Graph()\n",
    "with graph_cnn3.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #RMSprop Optimizer\n",
    "    train_step = optimize_method('rmsprop', cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 147.333\n",
      "Loss: 6.35991\n",
      "Loss: 0.734154\n",
      "Loss: 0.775099\n",
      "Loss: 0.213127\n",
      "Loss: 0.521301\n",
      "Loss: 0.184838\n",
      "Loss: 0.293952\n",
      "Loss: 0.188002\n",
      "Loss: 0.0245486\n",
      "Testing Accuracy： 0.9947\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 5\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn3) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                loss = sess.run(cross_entropy, feed_dict=feed_dict)\n",
    "                print('Loss:', loss)\n",
    "                \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_cnn4 = tf.Graph()\n",
    "with graph_cnn4.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer(x, keep_prob)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    \n",
    "    #Decaying learning rate\n",
    "    #global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    #starter_learning_rate = 0.001\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "    #Get the gradient\n",
    "    #tvars = tf.trainable_variables()\n",
    "    #grads, _ = tf.clip_by_global_norm(tf.gradients(cross_entropy, tvars), 5)\n",
    "    #Clip the gradient\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    #train_step = optimizer.apply_gradients(\n",
    "        #zip(grads, tvars),\n",
    "        #global_step=global_step)\n",
    "    #Define accuracy\n",
    "    train_step = tf.train.MomentumOptimizer(0.0001, 0.9).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:147.41211 Accuracy:0.10938\n",
      "Loss:21.90355 Accuracy:0.93750\n",
      "Loss:7.92793 Accuracy:0.96875\n",
      "Loss:1.93566 Accuracy:1.00000\n",
      "Loss:6.34537 Accuracy:0.98438\n",
      "Loss:0.72142 Accuracy:1.00000\n",
      "Loss:1.92262 Accuracy:1.00000\n",
      "Loss:1.01777 Accuracy:0.98438\n",
      "Loss:4.03196 Accuracy:0.98438\n",
      "Loss:5.12439 Accuracy:0.96875\n",
      "Loss:1.94573 Accuracy:0.98438\n",
      "Loss:2.04563 Accuracy:0.98438\n",
      "Loss:1.33935 Accuracy:0.98438\n",
      "Loss:2.25252 Accuracy:0.98438\n",
      "Loss:0.06013 Accuracy:1.00000\n",
      "Loss:0.82740 Accuracy:0.98438\n",
      "Loss:0.31411 Accuracy:1.00000\n",
      "Loss:0.12469 Accuracy:1.00000\n",
      "Loss:1.67208 Accuracy:0.98438\n",
      "Loss:0.88636 Accuracy:1.00000\n",
      "Loss:0.48820 Accuracy:1.00000\n",
      "Loss:0.31426 Accuracy:1.00000\n",
      "Loss:0.80277 Accuracy:1.00000\n",
      "Loss:0.29741 Accuracy:1.00000\n",
      "Loss:0.67373 Accuracy:1.00000\n",
      "Loss:0.10931 Accuracy:1.00000\n",
      "Loss:0.64140 Accuracy:1.00000\n",
      "Loss:1.32997 Accuracy:1.00000\n",
      "Loss:0.06049 Accuracy:1.00000\n",
      "Loss:0.52272 Accuracy:1.00000\n",
      "Loss:6.59680 Accuracy:0.98438\n",
      "Loss:0.02583 Accuracy:1.00000\n",
      "Loss:0.33761 Accuracy:1.00000\n",
      "Loss:0.66808 Accuracy:1.00000\n",
      "Loss:0.02707 Accuracy:1.00000\n",
      "Loss:0.73816 Accuracy:1.00000\n",
      "Loss:0.10058 Accuracy:1.00000\n",
      "Loss:0.01783 Accuracy:1.00000\n",
      "Loss:0.01162 Accuracy:1.00000\n",
      "Loss:0.03912 Accuracy:1.00000\n",
      "Loss:0.45529 Accuracy:1.00000\n",
      "Loss:0.04565 Accuracy:1.00000\n",
      "Loss:0.22528 Accuracy:1.00000\n",
      "Loss:0.20611 Accuracy:1.00000\n",
      "Testing Accuracy： 0.999\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 22\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=graph_cnn4) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                l, acc = sess.run([cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "                print('Loss:{:.5f}'.format(l) , 'Accuracy:{:.5f}'.format(acc))\n",
    "                \n",
    "    count = 0    \n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems momentum optimizer works well here. Note, **sometimes the predicted values approximate 0, and log(0) will be nan**, in order to avoid this case, we can use ``` tf.clip_by_value(y_predict, 1e-10, 1.0) ``` to make the predicted value locate in a range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Xavier Initializer\n",
    "\n",
    "In order to improve the performance of CNN, we can replace normal initialization with Xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "#Create weights of random distributed\n",
    "def weight_variable(shape):\n",
    "  return tf.Variable(initializer(shape), name='weights')\n",
    "\n",
    "#Create biases\n",
    "def bias_variable(shape):\n",
    "  return tf.Variable(initializer(shape), name='biases')\n",
    "\n",
    "#Convolutional function\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#Max pool function\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnnLayer2(x, keep_prob):\n",
    "    x_image = tf.reshape(x, [-1,28,28,1])\n",
    "    #First Conv\n",
    "    with tf.name_scope('hidden1'):\n",
    "        kernel_shape, bias_shape = [3, 3, 1, 32], [32] \n",
    "        h_pool1 = conv_relu_pool(x_image, kernel_shape, bias_shape)\n",
    "        \n",
    "    #Second Conv\n",
    "    with tf.name_scope('hidden2'):\n",
    "        kernel_shape, bias_shape = [3, 3, 32, 64], [64] \n",
    "        h_pool2 = conv_relu_pool(h_pool1, kernel_shape, bias_shape)\n",
    "    \n",
    "    #Fully Connected Layer\n",
    "    with tf.name_scope('fully_connected'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        #Dropout, to prevent against overfitting      \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    #Softmax Layer\n",
    "    with tf.name_scope('softmax_layer'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn_xavier = tf.Graph()\n",
    "with cnn_xavier.as_default() as g:\n",
    "    #Create input placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None,10])\n",
    "    #Define ropout probability placholder\n",
    "    keep_prob = tf.placeholder(\"float\")   \n",
    "\n",
    "    y = cnnLayer2(x, keep_prob)\n",
    "    #Note, in order to prevent nan appear, we \n",
    "    #need to control the range of predicted values\n",
    "    y = tf.clip_by_value(y, 1e-10, 1.0)\n",
    "    #Define cross-entropy as loss function\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "    train_step = tf.train.MomentumOptimizer(0.0001, 0.9).minimize(cross_entropy)\n",
    "    #Define accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:148.62834 Accuracy:0.09375\n",
      "Loss:9.23215 Accuracy:0.93750\n",
      "Loss:9.23427 Accuracy:0.96875\n",
      "Loss:2.13606 Accuracy:1.00000\n",
      "Loss:2.63412 Accuracy:0.98438\n",
      "Loss:3.77895 Accuracy:0.96875\n",
      "Loss:4.23011 Accuracy:0.98438\n",
      "Loss:2.52950 Accuracy:0.98438\n",
      "Loss:4.96692 Accuracy:0.95312\n",
      "Loss:0.38285 Accuracy:1.00000\n",
      "Loss:0.58528 Accuracy:1.00000\n",
      "Loss:0.35823 Accuracy:1.00000\n",
      "Loss:3.13176 Accuracy:0.98438\n",
      "Loss:0.29653 Accuracy:1.00000\n",
      "Loss:1.16656 Accuracy:0.98438\n",
      "Loss:3.52067 Accuracy:0.98438\n",
      "Loss:2.22508 Accuracy:0.98438\n",
      "Loss:0.07129 Accuracy:1.00000\n",
      "Loss:0.18338 Accuracy:1.00000\n",
      "Loss:0.38540 Accuracy:1.00000\n",
      "Loss:1.61193 Accuracy:0.98438\n",
      "Loss:0.55105 Accuracy:1.00000\n",
      "Loss:0.08754 Accuracy:1.00000\n",
      "Loss:0.69087 Accuracy:1.00000\n",
      "Loss:0.01934 Accuracy:1.00000\n",
      "Loss:0.31349 Accuracy:1.00000\n",
      "Loss:0.32467 Accuracy:1.00000\n",
      "Loss:0.42119 Accuracy:1.00000\n",
      "Loss:0.09420 Accuracy:1.00000\n",
      "Loss:0.08421 Accuracy:1.00000\n",
      "Loss:0.05759 Accuracy:1.00000\n",
      "Loss:0.24962 Accuracy:1.00000\n",
      "Loss:1.48187 Accuracy:0.98438\n",
      "Loss:0.06765 Accuracy:1.00000\n",
      "Loss:0.16269 Accuracy:1.00000\n",
      "Loss:0.04117 Accuracy:1.00000\n",
      "Loss:0.04862 Accuracy:1.00000\n",
      "Loss:0.05068 Accuracy:1.00000\n",
      "Loss:1.58623 Accuracy:0.98438\n",
      "Loss:0.33845 Accuracy:1.00000\n",
      "Testing Accuracy： 0.9992\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 20\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session\n",
    "with tf.Session(graph=cnn_xavier) as sess:\n",
    "    #Initialize variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    #Run on GTX960 GPU\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "            #Train\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            if step%500 == 0:\n",
    "                feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "                l, acc = sess.run([cross_entropy, accuracy], feed_dict=feed_dict)\n",
    "                print('Loss:{:.5f}'.format(l) , 'Accuracy:{:.5f}'.format(acc))\n",
    "                \n",
    "    count = 0  \n",
    "    #Due to the limited GPU memory, we have to split the testing data into batches\n",
    "    for _ in range(200):\n",
    "        batch_data, batch_labels = mnist.train.next_batch(50)\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 1}\n",
    "        cp = sess.run(correct_prediction, feed_dict=feed_dict)\n",
    "        count += np.sum(cp)\n",
    "    print(\"Testing Accuracy：\", count/n_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems Xavier initialization makes the training converge faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Clipping and Learning Rate decaying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes if there are too many layers, or the weights are not well initialized, or the learning ratio is not small enough, we may encounter gradient explosion or vanish. In order to avoid such case, we can use gradient clip.\n",
    "\n",
    "### 3.1 Single Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "learning_rate = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnnLayer(x):\n",
    "    '''\n",
    "    Create a Rnn layer\n",
    "    Treat each image as a series of vectors\n",
    "    the time step is n_step\n",
    "    the size of input vector is n_input\n",
    "    '''\n",
    "    # Transform X into series\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    x = tf.reshape(x, (-1, n_steps, n_input))\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # (batch_size, n_input)\n",
    "    x = tf.split(x, n_steps, 0)\n",
    "    with tf.variable_scope(\"rnn_lstm\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        with tf.variable_scope('rnn_layer'):\n",
    "            # Get lstm cell output\n",
    "            outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "            # Linear activation, using rnn inner loop last output\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    logits = tf.matmul(outputs[-1], weights) + biases\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_rnn = tf.Graph()\n",
    "with graph_rnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps*n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    y = rnnLayer(x)\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    \n",
    "    cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    starter_learning_rate = 0.005\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cross_entropy, tvars),\n",
    "                                      5)\n",
    "    #Decaying the learning rate every 1000 steps\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 1000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=cur_step)\n",
    "    #train_op = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.25427 Training Accuracy= 0.10938\n",
      "Loss:0.02669 Training Accuracy= 0.93750\n",
      "Loss:0.01500 Training Accuracy= 0.95312\n",
      "Loss:0.00914 Training Accuracy= 0.98438\n",
      "Loss:0.01127 Training Accuracy= 0.96875\n",
      "Loss:0.00199 Training Accuracy= 0.98438\n",
      "Loss:0.00157 Training Accuracy= 1.00000\n",
      "Loss:0.00075 Training Accuracy= 1.00000\n",
      "Loss:0.00660 Training Accuracy= 0.96875\n",
      "Loss:0.00045 Training Accuracy= 1.00000\n",
      "Loss:0.00411 Training Accuracy= 0.98438\n",
      "Loss:0.00101 Training Accuracy= 1.00000\n",
      "Loss:0.00190 Training Accuracy= 1.00000\n",
      "Loss:0.00030 Training Accuracy= 1.00000\n",
      "Loss:0.00138 Training Accuracy= 1.00000\n",
      "Loss:0.00045 Training Accuracy= 1.00000\n",
      "Loss:0.00758 Training Accuracy= 0.96875\n",
      "Loss:0.00032 Training Accuracy= 1.00000\n",
      "Loss:0.00075 Training Accuracy= 1.00000\n",
      "Loss:0.00035 Training Accuracy= 1.00000\n",
      "Loss:0.00094 Training Accuracy= 1.00000\n",
      "Loss:0.00086 Training Accuracy= 1.00000\n",
      "Loss:0.00013 Training Accuracy= 1.00000\n",
      "Loss:0.00006 Training Accuracy= 1.00000\n",
      "Loss:0.00017 Training Accuracy= 1.00000\n",
      "Loss:0.00043 Training Accuracy= 1.00000\n",
      "Loss:0.00159 Training Accuracy= 0.98438\n",
      "Loss:0.00867 Training Accuracy= 0.98438\n",
      "Loss:0.00006 Training Accuracy= 1.00000\n",
      "Loss:0.00009 Training Accuracy= 1.00000\n",
      "Loss:0.00110 Training Accuracy= 1.00000\n",
      "Loss:0.00302 Training Accuracy= 0.98438\n",
      "Loss:0.00026 Training Accuracy= 1.00000\n",
      "Loss:0.00001 Training Accuracy= 1.00000\n",
      "Loss:0.00001 Training Accuracy= 1.00000\n",
      "Loss:0.00037 Training Accuracy= 1.00000\n",
      "Loss:0.00213 Training Accuracy= 0.98438\n",
      "Loss:0.00116 Training Accuracy= 1.00000\n",
      "Loss:0.00002 Training Accuracy= 1.00000\n",
      "Loss:0.00039 Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9893\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "e_pochs = 20\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "display_step = 600\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_rnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for _ in range(e_pochs):\n",
    "        for step in np.arange(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "             # Run optimization op (backprop)\n",
    "            _, loss, acc = sess.run([train_op, cross_entropy, accuracy], feed_dict={x: batch_x, y_: batch_y})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                print('Loss:{:.5f}'.format(loss), \"Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-layer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a two-layer LSTM neural network with dropout method to prevent against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "n_input = 28\n",
    "n_steps = 28 # timesteps\n",
    "layer_num = 2 #LSTM layer number\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(keep_prob):\n",
    "    '''Create a basic lstm cell with dropout'''\n",
    "    cell = rnn.BasicLSTMCell(n_hidden, forget_bias=0)\n",
    "    cell = rnn.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multi_rnnLayer(x, keep_prob):\n",
    "    # Transform X into series\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    x = tf.reshape(x, (-1, n_steps, n_input))\n",
    "    # split input into 28 seqs with size(batch_size, input_size)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "    with tf.variable_scope(\"multi_rnn_lstm\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        cell = lstm_cell\n",
    "        #Add dropout to prevent against overfitting\n",
    "        #cell = rnn.DropoutWrapper(cell=cell(), input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "        #Create two layer LSTM network\n",
    "        #Note cell() creates different cells each time, this is very important\n",
    "        #DO NOT USE lstm_cell directly which will invoke parameter sharing problem\n",
    "        mlstm_cell = rnn.MultiRNNCell([cell(keep_prob) for _ in range(layer_num)], state_is_tuple=True)\n",
    "        with tf.variable_scope('multi_rnn_layer'):\n",
    "            # Get lstm cell output, using dynamic rnn, the input is one tensor\n",
    "            #outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "            #if we use static rnn, we need to unrollthe graphs and the tensor slices\n",
    "            #x must be a list of tensors\n",
    "            outputs, states = rnn.static_rnn(mlstm_cell, x, dtype=tf.float32)\n",
    "            #An alternative below\n",
    "            #outputs = []\n",
    "            #state = mlstm_cell.zero_state(batch_size,tf.float32)\n",
    "            #for time_step in range(n_steps):\n",
    "                #if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                #cell_output, state = mlstm_cell(x[:, time_step, :], state)\n",
    "                #outputs.append(cell_output)\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    logits = tf.matmul(outputs[-1], weights) + biases\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_multirnn = tf.Graph()\n",
    "with graph_multirnn.as_default():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps*n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob =tf.placeholder(tf.float32)\n",
    "\n",
    "    y = multi_rnnLayer(x, keep_prob)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = -tf.reduce_mean(y_* tf.log(y))\n",
    "    #optimizer = tf.train.MomentumOptimizer(0.0001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "    cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    starter_learning_rate = 0.005\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cross_entropy, tvars),\n",
    "                                      5)\n",
    "    #Decaying the learning rate every 1000 steps\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 1000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=cur_step)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(y_,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.21875\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.95312\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 0.96875\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 1.00000\n",
      "Training Accuracy= 0.98438\n",
      "Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = 20\n",
    "num_steps = int(55000/batch_size)\n",
    "display_step = 801\n",
    "# Launch the graph\n",
    "with tf.Session(graph=graph_multirnn) as sess:\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # Keep training until reach max iterations\n",
    "    for _ in range(epochs):\n",
    "        for step in np.arange(num_steps):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y, keep_prob:1.0})\n",
    "                print(\"Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_data = mnist.test.images\n",
    "    test_label = mnist.test.labels\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y_: test_label, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 Cudnn RNN Model\n",
    "\n",
    "Cudnn RNN was introduced recently, and said to be able to improve performance. However it doesn't work well on my win10, the issue may be solved for later versions of Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 128\n",
    "display_step = 500\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "n_layer = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CudnnRnnLayer(x, keep_prob, is_training):\n",
    "\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshape to (n_steps*batch_size, n_input)\n",
    "    #x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    #x = tf.split(x, n_steps, 0)\n",
    "    \n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    with tf.variable_scope('CudnnRnnLayer'):\n",
    "        cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=n_layer, num_units =n_hidden, \n",
    "                                              input_size=n_input, direction = 'unidirectional',\n",
    "                                              input_mode='auto_select', dropout=0, seed=0)\n",
    "        params_size_t = cell.params_size()\n",
    "        rnn_params = tf.get_variable(\"lstm_params\", initializer=tf.random_uniform(\n",
    "            [params_size_t], -0.01, 0.01),  validate_shape=False)\n",
    "        c = tf.zeros([n_layers, batch_size, n_hidden],\n",
    "                 tf.float32)\n",
    "        h = tf.zeros([n_layers, batch_size, n_hidden],\n",
    "                 tf.float32)\n",
    "        initial_state = (tf.contrib.rnn.LSTMStateTuple(h=h, c=c),)\n",
    "        #   Get lstm cell output\n",
    "        with tf.variable_scope('Rnn_Structure'):\n",
    "                outputs, h, c = cell(x, h, c, rnn_params, is_training)\n",
    "                outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "                outputs = tf.reshape(outputs, [-1, n_hidden])\n",
    "        with tf.name_scope('fully_connected'):\n",
    "            # FUlly Connected layer\n",
    "            weights = tf.Variable(tf.random_normal([n_hidden, n_classes]), name='weights')\n",
    "            biases = tf.Variable(tf.random_normal([n_classes]), name='biases')\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.add(tf.matmul(outputs, weights), biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   }
    }
   ],
   "source": [
    "graph_CudnnRnn = tf.Graph()\n",
    "with graph_CudnnRnn.as_default():\n",
    "    # placeholder input\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    #Create a Cudnn  rnn layer\n",
    "    pred = CudnnRnnLayer(x, keep_prob, is_training)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    \n",
    "    #cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    #starter_learning_rate = 0.005\n",
    "    #tvars = tf.trainable_variables()\n",
    "    #grads, _ = tf.clip_by_global_norm(\n",
    "        #tf.gradients(cross_entropy, tvars), 5)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(cur_step)\n",
    "    #learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 500, 0.90, staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #train_op = optimizer.apply_gradients(\n",
    "        #zip(grads, tvars),\n",
    "        #global_step=cur_step)\n",
    "    train_op = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
